{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97472ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..............................................................................] 14406 / 14406C:\\data\\self\\phd\\new_laptop\\v4\\v1-ESWA\\streams-arff-to-missing-realworld\\noaa_temp.csv\n",
      "(5698, 8)\n",
      "(5698, 8)\n",
      "(5698, 8)\n",
      "(5698, 8)\n",
      "(5698, 8)\n",
      "(5698, 8)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/BorisMuzellec/MissingDataOT/master/utils.py')\n",
    "import numpy as np\n",
    "#from utils import *\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "def produce_NA(X, p_miss, mecha=\"MCAR\", opt=None, p_obs=None, q=None):\n",
    "    \"\"\"\n",
    "    Generate missing values for specifics missing-data mechanism and proportion of missing values. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data for which missing values will be simulated.\n",
    "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
    "    p_miss : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    mecha : str, \n",
    "            Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\", \"MNAR\" or \"MNARsmask\"\n",
    "    opt: str, \n",
    "         For mecha = \"MNAR\", it indicates how the missing-data mechanism is generated: using a logistic regression (\"logistic\"), quantile censorship (\"quantile\") or logistic regression for generating a self-masked MNAR mechanism (\"selfmasked\").\n",
    "    p_obs : float\n",
    "            If mecha = \"MAR\", or mecha = \"MNAR\" with opt = \"logistic\" or \"quanti\", proportion of variables with *no* missing values that will be used for the logistic masking model.\n",
    "    q : float\n",
    "        If mecha = \"MNAR\" and opt = \"quanti\", quantile level at which the cuts should occur.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionnary containing:\n",
    "    'X_init': the initial data matrix.\n",
    "    'X_incomp': the data with the generated missing values.\n",
    "    'mask': a matrix indexing the generated missing values.s\n",
    "    \"\"\"\n",
    "    \n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = X.astype(np.float32)\n",
    "        X = torch.from_numpy(X)\n",
    "    \n",
    "    if mecha == \"MAR\":\n",
    "        mask = MAR_mask(X, p_miss, p_obs).double()\n",
    "    elif mecha == \"MNAR\" and opt == \"logistic\":\n",
    "        mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
    "    elif mecha == \"MNAR\" and opt == \"quantile\":\n",
    "        mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
    "    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n",
    "        mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
    "    else:\n",
    "        mask = (torch.rand(X.shape) < p_miss).double()\n",
    "    \n",
    "    X_nas = X.clone()\n",
    "    X_nas[mask.bool()] = np.nan\n",
    "    \n",
    "    return {'X_init': X.double(), 'X_incomp': X_nas.double(), 'mask': mask}\n",
    "\n",
    "\n",
    "def process_arff_file(file_path):\n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Find the index where '@data' appears\n",
    "    data_index = next(i for i, line in enumerate(lines) if line.startswith('@data'))\n",
    "    \n",
    "    \n",
    "    # Iterate over each line\n",
    "    for i in range(0,data_index):\n",
    "        if lines[i].startswith(\"@attribute class\"):  # Check if line starts with \"@attribute class\"\n",
    "            # If it does, replace it with \"@attribute class1\"\n",
    "            lines[i] = lines[i].replace(\"@attribute class {0.0,1.0}\", \"@attribute class {0,1}\")\n",
    "            lines[i] = lines[i].replace(\"@attribute class {positive,negative}\", \"@attribute class {0,1}\")\n",
    "            lines[i] = lines[i].replace(\"@attribute class {class1,class2}\", \"@attribute class {0,1}\")\n",
    "            lines[i] = lines[i].replace(\"@attribute class {groupA,groupB}\", \"@attribute class {0,1}\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    #save first (n+1) rows\n",
    "    lines_arff_header = lines[0:data_index+2]\n",
    "\n",
    "    lines = lines[data_index+2:]\n",
    "    lines = [line.rstrip(',') for line in lines]\n",
    "\n",
    "    lines_csv_file_path = file_path.replace('.arff', '_temp.csv')\n",
    "    with open(lines_csv_file_path, 'w') as lines_csv_file:\n",
    "        lines_csv_file.writelines(lines)\n",
    "        \n",
    "    print(lines_csv_file_path)\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(lines_csv_file_path, header=None)\n",
    "\n",
    "    \n",
    "    df.iloc[:, -1] = df.iloc[:, -1].astype(str)\n",
    "    # Replace values in the last column\n",
    "    df.iloc[:, -1].replace({\n",
    "        '0.0': '0',\n",
    "        '1.0': '1',\n",
    "        'positive': '0',\n",
    "        'negative': '1',\n",
    "        'class1': '0',\n",
    "        'class2': '1',\n",
    "        'groupA': '0',\n",
    "        'groupB': '1'\n",
    "    }, inplace=True)\n",
    "\n",
    "    os.remove(lines_csv_file_path)\n",
    "\n",
    "    csv_file_path = file_path.replace('.arff', '_processing.csv')\n",
    "    \n",
    "    df.insert(0, 'New_Column', range(1, len(df) + 1))\n",
    "    df.to_csv(csv_file_path, index=False,header=False)\n",
    "\n",
    "    majority_data = df[df.iloc[:, -1] == '0']\n",
    "    minority_data = df[df.iloc[:, -1] == '1']\n",
    "\n",
    "    first_column = minority_data.iloc[:, 0]  # First column, contains sorting index\n",
    "    first_column = first_column.reset_index(drop=True)\n",
    "\n",
    "    actual_minority_data = minority_data.iloc[:, 1:]  # All other columns containing actual minority data\n",
    "\n",
    "    \n",
    "    \n",
    "    perc_arr = [0.05,0.10,0.15,0.20,0.25,0.30]\n",
    "    perc_arr_for_file = [5,10,15,20,25,30]\n",
    "    for i in range (0,len(perc_arr)):\n",
    "        perc = perc_arr[i]\n",
    "        # Perform operations on the minority data to create missing values\n",
    "        X_miss_mcar = produce_NA(actual_minority_data.iloc[:, :-1].values, p_miss=perc, mecha=\"MCAR\")\n",
    "\n",
    "        #print(X_miss_mcar)\n",
    "\n",
    "        X_mcar = X_miss_mcar['X_incomp']\n",
    "        R_mcar = X_miss_mcar['mask']\n",
    "        t_np =X_mcar.numpy() #convert to Numpy array\n",
    "        df_temp = pd.DataFrame(t_np) #convert to a dataframe\n",
    "        print(df_temp.shape)\n",
    "        # Remove the first row\n",
    "        #df_temp = df_temp.drop(df_temp.index[0])\n",
    "\n",
    "        # Remove the first column\n",
    "        #df_temp = df_temp.drop(df_temp.columns[0], axis=1)\n",
    "        minority_data = pd.concat([first_column, df_temp.reset_index(drop=True)], axis=1)\n",
    "        minority_data[minority_data.columns[-1] + 1] = 1 #create class label\n",
    "        minority_data = minority_data.reset_index(drop=True)\n",
    "        majority_data = majority_data.reset_index(drop=True)\n",
    "        all_data = pd.concat([majority_data, minority_data], axis=0)\n",
    "        all_data = all_data.sort_values(by=all_data.columns[0])\n",
    "\n",
    "        all_data = all_data.iloc[:, 1:]  # Remove the sorting index column\n",
    "        \n",
    "        csv_file_path = file_path.replace('.arff', '_'+str(perc_arr_for_file[i])+'perc.csv')\n",
    "        all_data.to_csv(csv_file_path,header=False,index=False) #save to file\n",
    "        # Convert the DataFrame to a comma-separated string\n",
    "        df_csv_string = all_data.to_csv(index=False,header=False)\n",
    "\n",
    "        # Concatenate the lines from the ARFF file and the DataFrame CSV string\n",
    "        combined_string = ''.join(lines_arff_header) + '\\n' + df_csv_string\n",
    "\n",
    "        # Write the combined string to a file\n",
    "        arff_file_path = file_path.replace('.arff', '_'+str(perc_arr_for_file[i])+'perc.arff')\n",
    "        with open(arff_file_path, 'w') as f:\n",
    "            f.write(combined_string)\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    #sys.exit()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #read the CSV file again\n",
    "    \n",
    "def process_folder(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    # Process each file\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.arff'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            process_arff_file(file_path)\n",
    "\n",
    "# Replace 'folder_path' with the path to your folder containing .arff files\n",
    "folder_path = ''\n",
    "process_folder(folder_path)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27390f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e094d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
