{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e968b27-7407-443c-94e6-3a6532799095",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Bin.INI : An Ensemble Approach for Dynamic Data Streams #############################\n",
    "################################# Authors: Muhammad Usman, Huanhuan Chen ####################################\n",
    "############################## For any queries: muhammadusman@mail.ustc.edu.cn ##############################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from time import perf_counter\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import statistics\n",
    "from decimal import Decimal\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn import preprocessing\n",
    "from numpy import sqrt\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math \n",
    "from numpy.linalg import norm\n",
    "import statistics\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial  import distance_matrix\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from efficient_apriori import apriori\n",
    "#from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\n",
    "from math import log2\n",
    "import uuid\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from pandas._libs.interval import Interval\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.8f}'.format)\n",
    "#np.set_printoptions(suppress=True)\n",
    "#np.set_printoptions(threshold=1000000000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def __get_coefficients(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = 0, 0, 0, 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_pred_a[i] == y_true[i] and y_pred_b[i] == y_true[i]:\n",
    "            a = a + 1\n",
    "        elif y_pred_a[i] != y_true[i] and y_pred_b[i] == y_true[i]:\n",
    "            b = b + 1\n",
    "        elif y_pred_a[i] == y_true[i] and y_pred_b[i] != y_true[i]:\n",
    "            c = c + 1\n",
    "        else:\n",
    "            d = d + 1\n",
    " \n",
    "    return a, b, c, d\n",
    "\n",
    "def q_statistics(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    q = float(a * d - b * c) / (a * d + b * c)\n",
    "    return q\n",
    "\n",
    "def double_fault_measure(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    df = float(d) / (a + b + c + d)\n",
    "    return df\n",
    "\n",
    "def disagreement_measure(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    disagreement = float(b + c) / (a + b + c + d)\n",
    "    return disagreement\n",
    "\n",
    "\n",
    "def custom_transform(original_value):\n",
    "    a = 1.0\n",
    "    transformed_value = a * (1 - original_value)\n",
    "    return transformed_value\n",
    "\n",
    "def cal_diversities(y_true, y_pred_a, y_pred_b):\n",
    "    \n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    \n",
    "    q = float(a * d - b * c) / (a * d + b * c)\n",
    "    \n",
    "    df = float(d) / (a + b + c + d)\n",
    "    \n",
    "    da = float(b + c) / (a + b + c + d)\n",
    "    \n",
    "    #print(\"original-\")\n",
    "    #print(q)\n",
    "    #print(df) \n",
    "    #print(da)   \n",
    "    q = custom_transform(q)\n",
    "    df = 1 - df\n",
    "    #print(\"some error\")\n",
    "    #print(\"transformed-\")\n",
    "    ##print(q)\n",
    "    #print(df) \n",
    "    #print(da)   \n",
    "    # Find the minimum and maximum values\n",
    "    min_value = min(q, df, da)\n",
    "    max_value = max(q, df, da)\n",
    "\n",
    "    # Normalize each value\n",
    "    q = min_max_normalize(q, min_value, max_value)\n",
    "    df = min_max_normalize(df, min_value, max_value)\n",
    "    da = min_max_normalize(da, min_value, max_value)\n",
    "    \n",
    "    #print(\"normalized-\")\n",
    "    #print(q)\n",
    "    #print(df) \n",
    "    #print(da)   \n",
    "    return q,df,da\n",
    "\n",
    "\n",
    "def min_max_normalize(value, min_val, max_val):\n",
    "    return (value - min_val) / (max_val - min_val)\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 += str(ele)+ \"-\" \n",
    "    \n",
    "    # return string  \n",
    "    return str1 \n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "def pltcolor(lst):\n",
    "    cols=[]\n",
    "    for l in lst:\n",
    "        if l==0:\n",
    "            cols.append('blue')\n",
    "        elif l==1:\n",
    "            cols.append('red')\n",
    "    return cols\n",
    "\n",
    "\n",
    "def detect_overlap(data_min,data_maj,n_features):\n",
    "    fishers = []\n",
    "    found = 0\n",
    "    max_f1 = 0\n",
    "    max_feature_index = 0\n",
    "    Xy_up = data_min\n",
    "    Xy_down = data_maj\n",
    "    frames = [Xy_up,Xy_down]\n",
    "    Xy_d = pd.concat(frames)\n",
    "    f_data_up_means = Xy_up.mean()\n",
    "    f_data_down_means= Xy_down.mean()\n",
    "    f_data_up_vars = Xy_up.var()\n",
    "    f_data_down_vars =Xy_down.var()\n",
    "    for i in range(n_features):\n",
    "        mean_up = f_data_up_means[i]\n",
    "        mean_down = f_data_down_means[i]\n",
    "        var_up = f_data_up_vars[i]\n",
    "        var_down = f_data_down_vars[i]\n",
    "        fisher = ((mean_up-mean_down)**2) / (var_up + var_down)\n",
    "        fishers.append([i,round(fisher,5),round(mean_up,5),round(mean_down,5)])\n",
    "        if(fisher>max_f1) :\n",
    "            max_f1 = fisher\n",
    "            max_feature_index = i\n",
    "\n",
    "    max_index= max_feature_index + 1\n",
    "    #print(\"Actual Max:\",max_f1)\n",
    "    max_f1 = 1/(1+max_f1)   # push between (0,1]\n",
    "    #print(\"Transformed Max:\",max_f1)       \n",
    "    fishers=sorted(fishers,key=lambda x: x[1], reverse=False)\n",
    "    return round(max_f1,5), fishers\n",
    "\n",
    "def update_minority_buffer(X,y,n_features):\n",
    "    global minority_buffer_x\n",
    "    global minority_buffer_y \n",
    "    global w\n",
    "    global df_buffer_x\n",
    "    global min_life_max\n",
    "    \n",
    "    decrease_life()   \n",
    "    remove_zero_life_rows()\n",
    "    for n in range (0, len(X)):\n",
    "        df_buffer_x = df_buffer_x.append(X.iloc[n])\n",
    "        df_buffer_x.iloc[-1,df_buffer_x.columns.get_loc(\"life\")] = min_life_max\n",
    "        df_buffer_x.iloc[-1,df_buffer_x.columns.get_loc(\"recall_weight\")] = 1\n",
    "        df_buffer_x.iloc[-1,df_buffer_x.columns.get_loc(\"total_weight\")] = 1\n",
    "    recalculate_total_weight()\n",
    "\n",
    "def decrease_life():\n",
    "    global df_buffer_x\n",
    "    global min_life_decay_factor    \n",
    "    df_buffer_x['life'] -= min_life_decay_factor\n",
    "   # print(df_buffer_x)\n",
    "\n",
    "def update_recall_weight(weight):\n",
    "    global df_buffer_x\n",
    "    global min_life_max\n",
    "    df_buffer_x.loc[df_buffer_x.life == min_life_max, 'recall_weight'] = weight   \n",
    "    #print(df_buffer_x)\n",
    "   # sys.exit()\n",
    "    \n",
    "def recalculate_total_weight():\n",
    "    global df_buffer_x\n",
    "    global min_life_max\n",
    "    df_buffer_x['total_weight'] = 0.5* df_buffer_x['life'] + 0.5* df_buffer_x['recall_weight']   \n",
    "    #print(df_buffer_x)\n",
    "    \n",
    "#performs random oversampling  \n",
    "def resample_from_buffer(X,y,samples_to_add):\n",
    "   # print(X)\n",
    "    global df_buffer_x\n",
    "    temp_X = X\n",
    "    temp_y = y\n",
    "    temp_y = temp_y.iloc[0:0]\n",
    "    temp_X = temp_X.iloc[0:0]\n",
    "    \n",
    "    to_add_for_one_element = round(samples_to_add/len(df_buffer_x),0)\n",
    "    #print(\"to_add_for_one_element:\",to_add_for_one_element)\n",
    "    min_buf_copy = df_buffer_x\n",
    "    min_buf_copy = min_buf_copy.drop(['life', 'recall_weight','total_weight'], axis=1)\n",
    "    #print(len(min_buf_copy))\n",
    "    for v in range (0,len(min_buf_copy)):\n",
    "        #print(X.iloc[[v]])\n",
    "        for c in range(0,int(to_add_for_one_element)):\n",
    "            #print(\"adding resample\")\n",
    "            temp_X = temp_X.append(min_buf_copy.iloc[[v]])\n",
    "            temp_y = temp_y.append({'class': 1}, ignore_index=True)\n",
    "            #print(len(temp_X))\n",
    "    #print(temp_X)\n",
    "    #print(\"resample from buffer--return rows\")\n",
    "    #print(temp_X)\n",
    "    #print(temp_y)\n",
    "    \n",
    "   # print(len(temp_X))\n",
    "    #print(len(temp_y))\n",
    "    return temp_X,temp_y\n",
    "\n",
    "def remove_zero_life_rows():\n",
    "    global df_buffer_x\n",
    "    #print(\"before:\",len(df_buffer_x))  \n",
    "    df_buffer_x = df_buffer_x[df_buffer_x['total_weight'] >= 0.50]\n",
    "    #print(\"after:\",len(df_buffer_x))\n",
    "\n",
    "\n",
    "#performs random oversampling  \n",
    "def get_weighted_buffer_data(X,y):\n",
    "    global df_buffer_x\n",
    "    temp_X = X\n",
    "    temp_y = y\n",
    "    temp_y = temp_y.iloc[0:0]\n",
    "    temp_X = temp_X.iloc[0:0]\n",
    "    min_buf_copy = df_buffer_x #.loc[df_buffer_x['total_weight'] >= 0.50] \n",
    "    min_buf_copy = min_buf_copy.drop(['life', 'recall_weight','total_weight'], axis=1)    \n",
    "    for v in range (0,len(min_buf_copy)):\n",
    "            temp_X = temp_X.append(min_buf_copy.iloc[[v]])\n",
    "            temp_y = temp_y.append({'class': 1}, ignore_index=True)    \n",
    "    return temp_X,temp_y\n",
    "\n",
    "#code for fixing clusters in the data\n",
    "#user class_to_process=1 for minority space -- creates equal size clusters.\n",
    "#for majority, set class_to_process=0, performs sampling within clusters without making them equal size.\n",
    "def fix_disjuncts(X,y,samples_to_add,class_to_process,DontAddToBuffer,n_features):\n",
    "    global disjuncts\n",
    "    global output_folder\n",
    "    global disjuncts_threshold\n",
    "    global arr_ord_cols\n",
    "    global minority_buffer_x\n",
    "    global minority_buffer_y\n",
    "    global w\n",
    "    global df_buffer_x\n",
    "    #if minority is less than 5% in the batch \n",
    "    #then use minority buffer to do get data.    \n",
    "    min_min_count = int(w*0.05) #we need atleast 5% samples for clustering\n",
    "    \n",
    "     \n",
    "\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    #print(X)\n",
    "    if(len(X) < 10):\n",
    "        return X,y\n",
    "    \n",
    "    data_scaled = X\n",
    "    n_samples, n_features = X.shape\n",
    "    try:\n",
    "        data_scaled = preprocessing.scale(X)\n",
    "    except:\n",
    "        return X,y\n",
    "    linked = linkage(data_scaled, 'average')\n",
    "    last = linked[-10:, 2]\n",
    "    last_rev = last[::-1]\n",
    "    idxs = np.arange(1, len(last) + 1)\n",
    "    accele = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "    accele_rev = accele[::-1]\n",
    "    k = accele_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "\n",
    "    model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='average', memory=None, n_clusters=k)\n",
    "    y_hc=model.fit_predict(X)\n",
    "    aggloclust=model.fit(X)\n",
    "    (unique, counts) = np.unique(aggloclust.labels_, return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts)).T\n",
    "    frequencies = frequencies[np.argsort(frequencies[:, 1])[::-1]]\n",
    "    arr_cumsum = np.cumsum(frequencies,axis=0)\n",
    "    cumsum = arr_cumsum[:,1]\n",
    "    arr_sum_cumsum = np.sum(frequencies,axis=0)\n",
    "    arr_clusters = []\n",
    "    sum_cumsum = arr_sum_cumsum[1]\n",
    "    for i in range(len(arr_cumsum)):\n",
    "        val = cumsum[i]\n",
    "        arr_clusters.append(frequencies[i,0])\n",
    "\n",
    "    \n",
    "    arr_y_frames  = []\n",
    "    arr_x_frames  = []\n",
    "    cluster1_count =0\n",
    "    df_x1 = []\n",
    "    df_y1 = []\n",
    "    \n",
    "    disjuncts = len(arr_clusters)\n",
    "    if(disjuncts<disjuncts_threshold):\n",
    "        return X,y\n",
    "    \n",
    "    valid_clusters =0\n",
    "    for i in range(len(arr_clusters)):\n",
    "        arr_indexes = ((np.where(y_hc == int(arr_clusters[i]))))\n",
    "        df_y = y.loc[y.index[arr_indexes]]\n",
    "        if(len(df_y) > 2):\n",
    "            valid_clusters += 1\n",
    "    if(valid_clusters == 0):\n",
    "        return X,y\n",
    "    \n",
    "    each_cluster_addition = int(np.round(samples_to_add / valid_clusters ))  \n",
    "    for i in range(len(arr_clusters)):\n",
    "            arr_indexes = ((np.where(y_hc == int(arr_clusters[i]))))\n",
    "            df_x = X.loc[X.index[arr_indexes]]\n",
    "            df_y = y.loc[y.index[arr_indexes]]\n",
    "            \n",
    "            \n",
    "            if(len(df_x) <3):\n",
    "                continue\n",
    "            \n",
    "            to_add = 0\n",
    "            if(i==0):\n",
    "                cluster1_count = len(df_x)\n",
    "                to_add = each_cluster_addition\n",
    "            else:\n",
    "                to_add = each_cluster_addition\n",
    "                \n",
    "                #code to make equal size clusters in minority\n",
    "                if(class_to_process == 1):\n",
    "                    current_cluster_count = len(df_x)\n",
    "                    to_add += (cluster1_count-current_cluster_count)\n",
    "                   \n",
    "            df_cov = df_x.cov()\n",
    "            df_mean = df_x.mean(axis=0)\n",
    "            try:\n",
    "                new_samples = np.random.multivariate_normal(df_mean, df_cov,to_add)\n",
    "            except:\n",
    "                return X,y\n",
    "            \n",
    "            cols = []\n",
    "            for f in range(1,n_features+1):\n",
    "                cols.append(str(f))\n",
    "            df_new_samples = pd.DataFrame(new_samples,columns=cols)\n",
    "            df_x = pd.concat([df_x.reset_index(drop=True),df_new_samples.reset_index(drop=True)],axis=0)\n",
    "            df_x = df_x.reset_index(drop=True)\n",
    "            if(len(arr_ord_cols)>0):\n",
    "                for h in range (0,len(arr_ord_cols)):\n",
    "                    ix = arr_ord_cols[h]\n",
    "                    df_x[str(ix)] = round(df_x[str(ix)]) \n",
    "            for j in range(0,to_add):\n",
    "                df_y = df_y.append(pd.Series([class_to_process],index=['class']),ignore_index = True) \n",
    "            \n",
    "            df_y = df_y.reset_index(drop=True)            \n",
    "            arr_x_frames.append(df_x)\n",
    "            arr_y_frames.append(df_y)        \n",
    "    \n",
    "    np.set_printoptions(threshold=np.inf)            \n",
    "    modified_X = pd.concat(arr_x_frames)\n",
    "    modified_Y = pd.concat(arr_y_frames)\n",
    "    \n",
    "    modified_X = modified_X.reset_index(drop=True)\n",
    "    modified_Y = modified_Y.reset_index(drop=True)\n",
    "    #print(\"**********</Fix Disjuncts End>************\")\n",
    "    #print(modified_X.shape)\n",
    "    return modified_X,modified_Y    \n",
    "\n",
    "\n",
    "\n",
    "def icrc(X_maj,X_min,y_maj,y_min,n_features,overlap_val,fullFit,ups,downs):\n",
    "    \n",
    "    \n",
    "    \n",
    "    global f1_overlap_value\n",
    "    global overlap_threshold_f1\n",
    "    global imbalance_ratio_threshold\n",
    "    ##################### Update Imbalance status of current window\n",
    "    is_imbalance = False\n",
    "    ir = 0\n",
    "    if(ups ==0 or downs ==0):\n",
    "        ir = imbalance_ratio_threshold + 1\n",
    "    elif(ups > downs):\n",
    "        ir = ups/downs\n",
    "    elif(ups < downs):\n",
    "        ir = downs/ups\n",
    "    elif(ups == downs):\n",
    "        ir = 1\n",
    "\n",
    "    if(ir>=imbalance_ratio_threshold):\n",
    "        is_imbalance = True\n",
    "    ########################################\n",
    "\n",
    "    #is_imbalance = False\n",
    "    if(is_imbalance == False):\n",
    "        frames = [X_min,X_maj]\n",
    "        modified_X = pd.concat(frames)\n",
    "        frames = [y_min,y_maj]\n",
    "        modified_Y = pd.concat(frames).replace('UP',1).replace('DOWN',0) \n",
    "        return modified_X.to_numpy(),modified_Y.to_numpy().reshape(-1)\n",
    "\n",
    "    if(is_imbalance==True):\n",
    "        is_overlap = False\n",
    "        fitted_already = False\n",
    "\n",
    "        \n",
    "        modified_Xmin,modified_Ymin = fix_disjuncts(X_min,y_min,(len(X_maj)-len(X_min)),1,fullFit,n_features)\n",
    "        modified_Xmaj= X_maj\n",
    "        modified_Ymaj = y_maj\n",
    "        f1_overlap_value = overlap_val\n",
    "        if (f1_overlap_value > overlap_threshold_f1): \n",
    "            #print(\"overlap true\")\n",
    "            is_overlap =  True\n",
    "        if is_overlap:\n",
    "            modified_Xmaj = modified_Xmaj.reset_index(drop=True)\n",
    "            modified_Xmin = modified_Xmin.reset_index(drop=True)\n",
    "            idx_maj = len(modified_Xmaj)\n",
    "\n",
    "            \n",
    "            all_data = pd.concat([modified_Xmaj,modified_Xmin])\n",
    "\n",
    "            a = pdist(all_data,metric='euclidean')\n",
    "\n",
    "            t_l = len(all_data)\n",
    "\n",
    "            d_1 = linkage(a)\n",
    "            \n",
    "            d_2 = pd.DataFrame(d_1,columns=['i1','i2','i3','i4'])\n",
    "            filtered_values = np.where( ((d_2[\"i1\"] < idx_maj) & (d_2[\"i2\"] > idx_maj) & (d_2[\"i2\"] <t_l)  ) | ((d_2[\"i1\"] > idx_maj) & (d_2[\"i1\"] < t_l ) & (d_2[\"i2\"] < idx_maj) ) )\n",
    "\n",
    "            filtered =  d_2.loc[filtered_values]\n",
    "            arr_indexes = filtered.iloc[:,0].to_numpy() \n",
    "\n",
    "            to_remove = int(len(arr_indexes)*disjunct_threshold_perc_maj)        \n",
    "            drop_these = (arr_indexes[0:to_remove])\n",
    "            modified_Xmaj.drop(index=drop_these,inplace=True)\n",
    "            modified_Ymaj = modified_Ymaj.iloc[to_remove:]\n",
    "\n",
    "\n",
    "        if(len(modified_Xmin) > len(modified_Xmaj)):\n",
    "            modified_Xmaj,modified_Ymaj = fix_disjuncts(modified_Xmaj,modified_Ymaj,(len(modified_Xmin)-len(modified_Xmaj)),0,fullFit,n_features)\n",
    "\n",
    "        frames = [modified_Xmin,modified_Xmaj]\n",
    "        modified_X = pd.concat(frames)\n",
    "        frames = [modified_Ymin,modified_Ymaj]\n",
    "        modified_Y = pd.concat(frames).replace('UP',1).replace('DOWN',0) \n",
    "        \n",
    "    return modified_X.to_numpy(),modified_Y.to_numpy().reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DetectorClassifier(BaseEstimator):\n",
    "    def __init__(self, clf, classes):\n",
    "        if not hasattr(clf, \"partial_fit\"):\n",
    "            raise TypeError(\"Choose incremental classifier\")\n",
    "        self.clf = clf\n",
    "        classes = [0,1]\n",
    "        self.classes = classes\n",
    "        self.change_detected = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #self.clf = clone(self.clf)\n",
    "        self.clf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self,modified_X,modified_Y,fullFit):\n",
    "            if(fullFit):\n",
    "               # print(\"^^^^Full Fit^^^\")    \n",
    "                self.clf = self.clf.fit(modified_X, modified_Y, classes=self.classes)            \n",
    "            else:\n",
    "               # print(\"^^^^Partial Fit^^^\")\n",
    "                self.clf = self.clf.partial_fit(modified_X, modified_Y, classes=self.classes)\n",
    "\n",
    "            return self\n",
    "    \n",
    "    ##### Prdict Function        \n",
    "    def predict(self, X):\n",
    "        return np.asarray(self.clf.predict(X), dtype=np.int64).ravel()\n",
    "    \n",
    "    ##### Score Function\n",
    "    def score(self, X,y):\n",
    "        return self.clf.score(X,y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X) \n",
    "    \n",
    "def bin_counts(data_length):\n",
    "    #Sturges' Rule\n",
    "    #num_bins = 1 + int(np.log2(data_length))\n",
    "    \n",
    "    #Sq. Root Rule\n",
    "    num_bins = int(sqrt(data_length))\n",
    "    return num_bins\n",
    "\n",
    "\n",
    "\n",
    "def doanes_formula(data):\n",
    "    n = len(data)\n",
    "    \n",
    "    #return bin_counts(n)\n",
    "    \n",
    "    # Calculate skewness and standard error of skewness\n",
    "    skewness = 3 * (np.mean(data) - np.median(data)) / np.std(data)\n",
    "    se_skewness = np.sqrt(6 * (n - 2) / ((n + 1) * (n + 3)))\n",
    "    \n",
    "    # Calculate number of bins using Doane's formula\n",
    "    if(skewness == 0):\n",
    "        num_bins = 1 + log2(n)\n",
    "    else:\n",
    "        \n",
    "        num_bins = 1 + log2(n) + log2(1 + abs(skewness) / se_skewness)\n",
    "    print(num_bins)\n",
    "    return int(num_bins)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def sigmoid_decay(t):\n",
    "    global initial_value\n",
    "    global final_value\n",
    "    global time_constant\n",
    "    return initial_value + (final_value - initial_value) / (1 + np.exp(-(t - time_constant) / time_constant))\n",
    "   \n",
    "    \n",
    "def sigmoid_decay_ensemble_weight(t):\n",
    "    global initial_value\n",
    "    global final_value\n",
    "    global time_constant_ensemble_weight\n",
    "    return initial_value + (final_value - initial_value) / (1 + np.exp(-(t - time_constant_ensemble_weight) / time_constant_ensemble_weight))\n",
    "   \n",
    "def calculate_cohens_kappa(actual, predicted):\n",
    "    # Confusion Matrix\n",
    "    tp = sum((a == 1) and (p == 1) for a, p in zip(actual, predicted))\n",
    "    tn = sum((a == 0) and (p == 0) for a, p in zip(actual, predicted))\n",
    "    fp = sum((a == 0) and (p == 1) for a, p in zip(actual, predicted))\n",
    "    fn = sum((a == 1) and (p == 0) for a, p in zip(actual, predicted))\n",
    "    \n",
    "    # Observed agreement\n",
    "    observed_agreement = (tp + tn) / len(actual)\n",
    "    \n",
    "    # Calculate expected agreement\n",
    "    expected_agreement = ((tp + fn) * (tp + fp) + (tn + fp) * (tn + fn)) / (len(actual) ** 2)\n",
    "    \n",
    "    # Calculate Kappa\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n",
    "    \n",
    "    return kappa\n",
    "       \n",
    "def prequential(window_size,Xy,X, y, clf,n_features,overlap_threshold,imbalance_ratio_threshold,disjunct_threshold_perc_maj,file_out_prefix, n_train=1):\n",
    "    \n",
    "    \"\"\"Prequential Evaluation: instances are first used to test, and then to train\n",
    "    :return the label predictions for each test instance, and the associated running time\n",
    "    \"\"\"\n",
    "    first_weights_flag = False\n",
    "    \n",
    "    arr_clf = []\n",
    "    arr_clf_temp_to_use = []\n",
    "    arr_weights = []\n",
    "    arr_recall0_cw = []\n",
    "    arr_recall1_cw = []\n",
    "    \n",
    "    a= 0.00001\n",
    "    \n",
    "    start = timer()\n",
    "\n",
    "    global arr_elapsedtime\n",
    "    global restart_threshold\n",
    "    global arr_recall_0\n",
    "    global arr_recall_1\n",
    "    global output_folder\n",
    "    global maj_weight\n",
    "    global min_weight\n",
    "    global ens_weight\n",
    "    global ensemble_pool_size    \n",
    "    global pruned_ensemble_size\n",
    "    global rl_pruned_ids\n",
    "    global preq_Qtable\n",
    "    global col_means\n",
    "    global imputed_from_arm\n",
    "    global total_miss_count\n",
    "    global all_rules_df\n",
    "    \n",
    "    global all_models_df\n",
    "    global arr_all_gmeans\n",
    "    global arr_all_recall0\n",
    "    global arr_all_recall1\n",
    "    global str_settings\n",
    "    global arr_all_perc_replaced\n",
    "    \n",
    "    global col_means\n",
    "    global imputed_missing_data\n",
    "    \n",
    "    global df_weights_history\n",
    "    \n",
    "    weights_pruned = []\n",
    "    arr_actual_win = []\n",
    "    arr_pred_win = []\n",
    "    arr_roc = []\n",
    "    arr_divs = []\n",
    "    ensemble_tp = 0\n",
    "    ensemble_fp =0\n",
    "    ensemble_fn = 0\n",
    "    ensemble_tn = 0\n",
    "    \n",
    "    \n",
    "    ensemble_correct =0\n",
    "    zeroClassCounter = 0\n",
    "    oneClassCounter = 0\n",
    "    current_correct_0 = 0\n",
    "    current_correct_1 = 0\n",
    "    \n",
    "    current_correct_ensemble_0 = 0\n",
    "    current_correct_ensemble_1 = 0\n",
    "    \n",
    "    predictions = \"\"    \n",
    "    row_num = y.shape[0]   \n",
    "   \n",
    "    # Split an init batch\n",
    "    X_init = X[0:n_train]\n",
    "    y_init = y[0:n_train]\n",
    "    \n",
    "    # Used for training and evaluation\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    Xy_train = Xy\n",
    "    \n",
    "        \n",
    "    ensemble_preds = np.zeros(row_num)\n",
    "    current_correct_0_cw = np.zeros(ensemble_pool_size)\n",
    "    current_correct_1_cw = np.zeros(ensemble_pool_size)\n",
    "    recall_0_cw = np.zeros(ensemble_pool_size)\n",
    "    recall_1_cw = np.zeros(ensemble_pool_size)\n",
    "    \n",
    "    rows_to_remove = np.any(np.isnan(X_init) | (X_init == None), axis=1)\n",
    "    X_init = X_init[~rows_to_remove]\n",
    "\n",
    "    col_means = np.mean(X_init,axis=0)\n",
    "    \n",
    "\n",
    "    indices_top_n = np.argsort(y_init)[-len(X_init):]\n",
    "\n",
    "    # Select the top n elements based on indices\n",
    "    top_n_elements = y_init[indices_top_n]\n",
    "    for g in range (0,ensemble_pool_size):\n",
    "        #Create a classifier and add it to the pool\n",
    "        obj_clf = DetectorClassifier(HoeffdingTreeClassifier(), [0,1])\n",
    "        \n",
    "        #print(top_n_elements)\n",
    "        obj_clf = obj_clf.fit(X_init,top_n_elements)\n",
    "        arr_clf.append(obj_clf)\n",
    "        arr_weights.append(1)\n",
    "        df_weights_history = df_weights_history.append({'i':g},ignore_index = True)\n",
    "        \n",
    "    ##print(df_weights_history)\n",
    "    \n",
    "    #sys.exit()\n",
    "    current_w = 0\n",
    "    window_counter = 0\n",
    "    strProcessing = \"\"\n",
    "    correct_predictions = 0\n",
    "\n",
    "    strout = \"\"\n",
    "    strgmean = \"\"\n",
    "\n",
    "    oneClassCounter_CWindow = 0\n",
    "    zeroClassCounter_CWindow = 0\n",
    "    arr_cw_ens_preds = []\n",
    "    current_correct_0_cw_est = 0\n",
    "    current_correct_1_cw_est = 0\n",
    "    window_recall_0 = 0\n",
    "    window_recall_1 = 0\n",
    "        \n",
    "    for i in range(0, row_num):\n",
    "        #print(X[i, :])\n",
    "        #sys.exit()\n",
    "        current_w += 1\n",
    "        start_time = perf_counter()\n",
    "        arr_preds = []\n",
    "        arr_temp_preds = []\n",
    "        row_process = impute_row(X_train[i, :],False)\n",
    "        for n in range(0,len(arr_clf)):\n",
    "            clf = arr_clf[n]\n",
    "            for_pred = row_process.reshape(1, -1)\n",
    "            try:\n",
    "                prediction_clf = clf.predict(for_pred)\n",
    "                arr_preds.append(prediction_clf[0])\n",
    "                arr_temp_preds.append(prediction_clf[0])\n",
    "            except:\n",
    "                print(\"predict:\",for_pred)\n",
    "                sys.exit()\n",
    "        arr_cw_ens_preds.append(arr_temp_preds)   \n",
    "        strout += \"------------\\n\"\n",
    "        estimators_arr = []\n",
    "        for n in range(0,len(arr_clf)):\n",
    "            estimators_arr.append((\"HT-\" + str(n), arr_clf[n]))\n",
    "        \n",
    "       \n",
    "        df_estimators_arr = pd.DataFrame(arr_clf)\n",
    "        \n",
    "        \n",
    "        arr_clf_temp_to_use = []\n",
    "        estimators_arr_temp = []\n",
    "        arr_weights_temp = []\n",
    "\n",
    "        ## Ensemble Pruning\n",
    "        for h in range(0,len(rl_pruned_ids)) :\n",
    "            try:\n",
    "                indexx = rl_pruned_ids[h].astype(int)\n",
    "            except:\n",
    "                indexx = rl_pruned_ids[h]\n",
    "            #print(\"using classifier on index:\",indexx)\n",
    "            arr_clf_temp_to_use.append(arr_clf[indexx])\n",
    "            estimators_arr_temp.append((\"HT-\" + str(indexx), arr_clf[indexx]))\n",
    "            #arr_weights_temp.append(arr_weights[indexx]) #old \n",
    "            arr_weights_temp.append(weights_pruned[h]) #received from clustering of classifiers\n",
    "        \n",
    "        if(len(arr_clf_temp_to_use) == 0):\n",
    "            for h in range(0,len(arr_clf)) :\n",
    "                arr_clf_temp_to_use.append(arr_clf[h])\n",
    "                estimators_arr_temp.append((\"HT-\" + str(h), arr_clf[h]))\n",
    "                arr_weights_temp.append(1)\n",
    "        pred = 0\n",
    "        try:\n",
    "            eclf = VotingClassifier(estimators = estimators_arr_temp, voting='soft',weights=arr_weights_temp) \n",
    "            eclf.estimators_ = arr_clf_temp_to_use\n",
    "            eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "            for_pred = row_process.reshape(1, -1)\n",
    "            #print(\"predict:\",for_pred)\n",
    "            pred_proba = eclf.predict_proba(for_pred)\n",
    "            pred = eclf.predict(for_pred)\n",
    "            arr_tempo = pred_proba[0]\n",
    "            arr_pred_win.append(pred)\n",
    "        except:\n",
    "            eclf = VotingClassifier(estimators = estimators_arr_temp, voting='hard',weights=arr_weights_temp) \n",
    "            eclf.estimators_ = arr_clf_temp_to_use\n",
    "            for_pred = row_process.reshape(1, -1)\n",
    "            #print(\"predict:\",for_pred)\n",
    "            eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "            pred = eclf.predict(for_pred)\n",
    "            if(pred==0):\n",
    "                arr_pred_win.append(0)\n",
    "            if(pred==1):\n",
    "                arr_pred_win.append(1)  \n",
    "        ensemble_preds[i] = pred\n",
    "        counter = i+1\n",
    "        lambda_0 = maj_weight#Majority\n",
    "        lambda_1 = min_weight #Minority\n",
    "        \n",
    "        if y_train[i] == 0:\n",
    "            zeroClassCounter += 1\n",
    "            zeroClassCounter_CWindow += 1\n",
    "        else: \n",
    "            oneClassCounter += 1\n",
    "            oneClassCounter_CWindow += 1\n",
    "        correct_found = 0\n",
    "\n",
    "        for n in range(0,len(arr_clf)):\n",
    "            if(y_train[i] == 0 and arr_preds[n] ==0):\n",
    "                current_correct_0_cw[n] = current_correct_0_cw[n]+1\n",
    "            if(y_train[i] == 1 and arr_preds[n] ==1):    \n",
    "                current_correct_1_cw[n] = current_correct_1_cw[n]+1\n",
    "        \n",
    "        correct_found = 0  \n",
    "        y_actual = 0\n",
    "        y_pred = 0\n",
    "        if(y_train[i] ==0 and pred ==0):\n",
    "            correct_found +=1\n",
    "            current_correct_ensemble_0 += 1\n",
    "            current_correct_0_cw_est += 1\n",
    "            ensemble_tn += 1\n",
    "            y_actual = 0\n",
    "            y_pred = 0\n",
    "        if(y_train[i] ==1 and pred == 1):\n",
    "            correct_found +=1\n",
    "            current_correct_ensemble_1 += 1\n",
    "            current_correct_1_cw_est += 1\n",
    "            ensemble_tp += 1\n",
    "            y_actual = 1\n",
    "            y_pred = 1\n",
    "        if(y_train[i] ==1 and pred ==0):\n",
    "            ensemble_fn += 1\n",
    "            y_actual = 1\n",
    "            y_pred = 0\n",
    "        if(y_train[i] ==0 and pred == 1):\n",
    "            ensemble_fp += 1\n",
    "            y_actual = 0\n",
    "            y_pred = 1\n",
    "        if(correct_found ==1):\n",
    "            ensemble_correct += 1\n",
    "        arr_actual_win.append(y_actual)\n",
    "        #when the batch is ready or an in complete batch at the end\n",
    "        if( ( current_w % window_size == 0)  or (current_w != window_size and i == (row_num - w -1)) ):\n",
    "\n",
    "            window_counter += 1\n",
    "            window_recall_0 = 0\n",
    "            window_recall_1 = 0\n",
    "            if zeroClassCounter_CWindow != 0:\n",
    "                window_recall_0 =  current_correct_0_cw_est / zeroClassCounter_CWindow\n",
    "            if oneClassCounter_CWindow != 0:\n",
    "                window_recall_1 =  current_correct_1_cw_est / oneClassCounter_CWindow\n",
    "            \n",
    "            window_gmean = sqrt(window_recall_0*window_recall_1)            \n",
    "            \n",
    "            update_recall_weight( (lambda_0*window_recall_0) + (lambda_1*window_recall_1))\n",
    "\n",
    "            kappa = cohen_kappa_score(arr_actual_win, arr_pred_win)\n",
    "            arr_kappa.append(kappa)\n",
    "            \n",
    "            arr_gmean.append(window_gmean*100)\n",
    "            arr_recall_0.append(window_recall_0*100)\n",
    "            arr_recall_1.append(window_recall_1*100)\n",
    "            \n",
    "            accuracy = (ensemble_tp+ensemble_tn)/(zeroClassCounter_CWindow+oneClassCounter_CWindow)\n",
    "    \n",
    "            arr_actual_win=[]\n",
    "            arr_pred_win = []\n",
    "                       \n",
    "            arr_accuracy.append(accuracy)\n",
    "            for n in range(0,len(arr_clf)):                    \n",
    "                if zeroClassCounter_CWindow != 0:\n",
    "                    recall_0_cw[n] =  current_correct_0_cw[n] / zeroClassCounter_CWindow                    \n",
    "                else:\n",
    "                    recall_0_cw[n] =  1\n",
    "                    \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                    if oneClassCounter_CWindow != 0:\n",
    "                        recall_1_cw[n] =  current_correct_1_cw[n] / oneClassCounter_CWindow\n",
    "                    else:\n",
    "                        recall_1_cw[n] = 1\n",
    "\n",
    "                \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                weight = (lambda_0 * recall_0_cw[n]) + (lambda_1*recall_1_cw[n])                \n",
    "                arr_weights[n] = weight\n",
    "                \n",
    "                row_to_update = df_weights_history[df_weights_history['i']==n]\n",
    "                for r in range (win_weight_count,1,-1):\n",
    "                    df_weights_history.loc[df_weights_history['i'] ==n, str(r)] = df_weights_history.loc[df_weights_history['i'] ==n, str(r-1)]   \n",
    "                df_weights_history.loc[df_weights_history['i'] ==n, '1'] = weight  \n",
    "\n",
    "            \n",
    "\n",
    "            arr_weights = normalize(arr_weights)\n",
    "            \n",
    "            oneClassCounter_CWindow = 0\n",
    "            zeroClassCounter_CWindow = 0  \n",
    "            current_correct_0_cw_est = 0\n",
    "            current_correct_1_cw_est = 0\n",
    "            ensemble_tn = 0\n",
    "            ensemble_tp = 0\n",
    "            ensemble_fp = 0\n",
    "            ensemble_fn = 0\n",
    "            \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                current_correct_0_cw[n]=0\n",
    "                current_correct_1_cw[n] = 0\n",
    "            \n",
    "            \n",
    "            x_preq = X_train[i-(window_size-1):i+1]\n",
    "            y_preq = y_train[i-(window_size-1):i+1]\n",
    "\n",
    "            Xy_preq = Xy_train[i-(window_size-1):i+1]\n",
    "            ups =  np.count_nonzero(y_preq == 1)\n",
    "            downs = np.count_nonzero(y_preq == 0)         \n",
    "\n",
    "            \n",
    "            arr_clf_temp = []\n",
    "\n",
    "            ####################### <Overlap Detection> ############################################\n",
    "            #Check overlaps where and send its status to partialfit\n",
    "            cols_str = []\n",
    "            for f in range(1,n_features+1):\n",
    "                cols_str.append(str(f))\n",
    "            cols_str.append('class')\n",
    "            Xy_d = pd.DataFrame(Xy_preq, columns = cols_str)\n",
    "\n",
    "            Xy_min = Xy_d[(Xy_d['class'] == 1)]\n",
    "            Xy_maj = Xy_d[(Xy_d['class'] == 0)]\n",
    "\n",
    "            cols = []\n",
    "            for f in range(0,n_features):\n",
    "                cols.append(f)\n",
    "            cols_class = [n_features]\n",
    "            X_min = Xy_min[Xy_min.columns[cols]]\n",
    "            X_maj = Xy_maj[Xy_maj.columns[cols]]\n",
    "            y_min = Xy_min[Xy_min.columns[cols_class]]\n",
    "            y_maj = Xy_maj[Xy_maj.columns[cols_class]]\n",
    "            \n",
    "            frames = [X_min,X_maj]\n",
    "            modified_X = pd.concat(frames)\n",
    "            frames = [y_min,y_maj]\n",
    "            modified_Y = pd.concat(frames)\n",
    "\n",
    "            cols_num = []            \n",
    "            for p in range(0,n_features):\n",
    "                cols_num.append(p)\n",
    "            m_X = modified_X\n",
    "            m_Y = modified_Y            \n",
    "            m_X.columns = cols_num\n",
    "            m_Y.columns = [0]  \n",
    "            \n",
    "            missing_data = X_min[X_min.isnull().any(axis=1)]\n",
    "            \n",
    "            \n",
    "            prev_col_means = col_means\n",
    "            col_means = X_min.mean(skipna=True).values \n",
    "            contains_nan_or_none = np.isnan(col_means).any()\n",
    "            #print(contains_nan_or_none)\n",
    "            if(contains_nan_or_none):\n",
    "                col_means = prev_col_means\n",
    "            do_regress(X_min,n_features,window_counter)\n",
    "            imputed_resamples = missing_data.copy()\n",
    "            imputed_resamples.drop(imputed_resamples.index).reset_index(drop=True)\n",
    "            \n",
    "            y_min_resampled = y_min.copy()\n",
    "            y_min_resampled = y_min_resampled.drop(y_min_resampled.index).reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            if(window_counter == 10000):\n",
    "                print(missing_data)\n",
    "                print(x_preq)\n",
    "                #sys.exit()\n",
    "            \n",
    "            for x in range (0,len(missing_data)):\n",
    "                a= impute_row(missing_data.iloc[x],True)\n",
    "                missing_data.iloc[x] = a\n",
    "                imputed_missing_data.append(missing_data.iloc[x])\n",
    "                    \n",
    "               \n",
    "            X_min_non_missing = X_min.dropna()\n",
    "            y_min_non_missing = y_min.head(len(X_min_non_missing))\n",
    "            \n",
    "            frames = [X_min_non_missing,missing_data]\n",
    "            X_min = pd.concat(frames)       \n",
    "            #print(\"x size b4:\",len(X_min))\n",
    "            for t in range (0, len(X_min)):\n",
    "                new_row = {'class': 1} \n",
    "                y_min_resampled.loc[len(y_min_resampled)] = new_row\n",
    "            \n",
    "            if(len(X_maj) ==0 or len(X_min_non_missing) == 0):\n",
    "                 overlap_val = 0                   \n",
    "            else:\n",
    "                overlap_val, fishers = detect_overlap(X_min_non_missing, X_maj,n_features)   \n",
    "            modified_x,modified_y = icrc(X_maj,X_min,y_maj,y_min_resampled,n_features,overlap_val,False,ups,downs)\n",
    "                   \n",
    "            \n",
    "            arr_temp_clfs = []\n",
    "            ## Partial Fit the previous classifiers\n",
    "            for n in range(0,len(arr_clf)):\n",
    "                clf = arr_clf[n]\n",
    "                clf = clf.partial_fit(modified_x,modified_y,False)\n",
    "                arr_temp_clfs.append(clf)\n",
    "             \n",
    "            \n",
    "            #################################### Commented out code that removes the minimum preto based member ###############\n",
    "            arr_clf = arr_temp_clfs\n",
    "            remove_index = 0\n",
    "            if(len(arr_clf) == ensemble_pool_size):\n",
    "                least_weight = arr_weights[0]\n",
    "                for q in range (1,len(arr_weights)-1):\n",
    "                    w_q = arr_weights[q]\n",
    "                    if(w_q < least_weight):\n",
    "                        remove_index = q\n",
    "                #print(\"removing:\",remove_index)\n",
    "                arr_clf = np.delete(arr_clf,remove_index)\n",
    "                arr_weights = np.delete(arr_weights,remove_index)\n",
    "            \n",
    "            for o in range(remove_index,ensemble_pool_size-1):\n",
    "                #print(\"o:\",o)\n",
    "                #print(df_weights_history.loc[df_weights_history['i'] ==o, str(1)])\n",
    "                \n",
    "                for r in range (1,win_weight_count+1):\n",
    "                    df_weights_history.loc[df_weights_history['i'] ==o, str(r)] = df_weights_history.loc[df_weights_history['i'] ==(o+1), str(r)].iloc[0]  \n",
    "                    \n",
    "            df_weights_history.loc[df_weights_history['i'] == (ensemble_pool_size-1), str('1')] = 1  \n",
    "            for r in range (2,win_weight_count+1):\n",
    "                df_weights_history.loc[df_weights_history['i'] == (ensemble_pool_size-1), str(r)] = np.nan\n",
    "            \n",
    "            \n",
    "            arr_weights_decayed = []\n",
    "            for o in range(0,ensemble_pool_size): \n",
    "                #print(\"---\\n\\n next member:\")\n",
    "                weight = 0\n",
    "                for r in range (1,win_weight_count+1):\n",
    "                    val = df_weights_history.loc[df_weights_history['i'] == (o), str(r)].iloc[0]\n",
    "                    if( np.isnan(val) == False ) :\n",
    "                        #print(\"actual val:\",val)\n",
    "                        decayed = sigmoid_decay_ensemble_weight(r) * val\n",
    "                        weight += decayed\n",
    "                arr_weights_decayed.append(weight)\n",
    "            ######################### add new classifier to the pool after training it with the current window.#####################\n",
    "            \n",
    "            obj_clf1 = DetectorClassifier(HoeffdingTreeClassifier(), [0,1])\n",
    "            clf_temp =obj_clf1.partial_fit(modified_x,modified_y,True)\n",
    " \n",
    "            arr_weights = np.append(arr_weights,1)\n",
    "            arr_clf = np.append(arr_clf,clf_temp)\n",
    "\n",
    "            \n",
    "            '''### diversity work '''\n",
    "            arr_temp = np.vstack(arr_cw_ens_preds) \n",
    "            arr_temp = np.delete(arr_temp,remove_index,axis=1)\n",
    "            arr_div_q = []\n",
    "            arr_div_df = []            \n",
    "            arr_div_da = []            \n",
    "            \n",
    "            for m in range(0,len(arr_clf)-1):\n",
    "                dq_sum = 0\n",
    "                ddf_sum = 0\n",
    "                dda_sum = 0\n",
    "                \n",
    "                for n in range(0,len(arr_clf)-1):\n",
    "                    if(m == n):\n",
    "                        continue\n",
    "                    ens_preds_a = arr_temp[:,m]\n",
    "                    ens_preds_b = arr_temp[:,n]\n",
    "                    dq= 1\n",
    "                    ddf = 1\n",
    "                    dda = 1\n",
    "                    try:\n",
    "                        \n",
    "                        dq,ddf,dda = cal_diversities(y_preq, ens_preds_a, ens_preds_b)\n",
    "                    except: \n",
    "                        \n",
    "                        ddq,ddf,dda = 1,1,1\n",
    "                    dq_sum += dq\n",
    "                    ddf_sum += ddf\n",
    "                    dda_sum += dda\n",
    "                    \n",
    "                avg_dq = 0    \n",
    "                avg_ddf = 0\n",
    "                avg_dda = 0    \n",
    "                \n",
    "                if(len(arr_clf) != 1):\n",
    "                    #average over all \n",
    "                    avg_dq = (dq_sum)/(len(arr_clf)-2)      \n",
    "                    avg_ddf = (ddf_sum)/(len(arr_clf)-2)                  \n",
    "                    avg_dda = (dda_sum)/(len(arr_clf)-2)                  \n",
    "                    \n",
    "                    \n",
    "                arr_div_q.append(avg_dq)\n",
    "                arr_div_df.append(avg_ddf)\n",
    "                arr_div_da.append(avg_dda)                \n",
    "                \n",
    "            arr_cw_ens_preds = [] \n",
    "            rl_pruned_ids = []   \n",
    "            #Apply clustering on diversities\n",
    "            df_clfs = pd.DataFrame( columns =['index','dq','ddf','dda','weight','final_weight'])\n",
    "            for v in range(0,len(arr_div_q)):\n",
    "\n",
    "                df_clfs = df_clfs.append({'index':v,'dq' : arr_div_q[v], 'ddf' : arr_div_df[v],'dda' : arr_div_da[v],'weight' : arr_weights_decayed[v],'final_weight' : 1},ignore_index = True)\n",
    "\n",
    "            df_clfs = df_clfs.append({'index':len(arr_div_q),'dq' : 0.5, 'ddf' : 0.5,'dda' : 0.5,'weight' : 0.5,'final_weight' : 0.5},ignore_index = True)\n",
    "            \n",
    "            rl_pruned_ids,weights_pruned = cluster_classifiers(df_clfs)\n",
    "            \n",
    "            '''### diversity work ends here '''\n",
    "            \n",
    "            \n",
    "             global pruned_ensemble_size\n",
    "             \n",
    "            ##########################################################################################################        \n",
    "            global model_interval\n",
    "            current_w = 0\n",
    "            prev_win_x = x_preq\n",
    "            prev_win_y =  y_preq\n",
    "            \n",
    "            end = timer()\n",
    "            elapsed_time = end - start\n",
    "            arr_elapsedtime.append(elapsed_time) \n",
    "            print(str(window_counter) + \"-\"+str(np.mean(arr_gmean)) + \"  --- \" + str(np.mean(arr_kappa)) + \"  --- \" + str(elapsed_time))\n",
    "                #sys.exit()\n",
    "            #   sys.exit()\n",
    "            \n",
    "    strout += \"\\n total corrects:\" + str(ensemble_correct)\n",
    "    strout += \"\\n zero+one:\" + str(zeroClassCounter+oneClassCounter)\n",
    "    np.savetxt(output_folder + file_out_prefix  +'.csv',  np.c_[arr_recall_0,arr_recall_1,arr_gmean,arr_elapsedtime,arr_accuracy,arr_kappa], delimiter=',',fmt='%1.2f')\n",
    "    return  np.mean(arr_recall_0), np.mean(arr_recall_1),np.mean(arr_gmean)\n",
    "\n",
    "\n",
    "def do_regress(X_min,n_features,window_counter):\n",
    "    epochs =1\n",
    "    global all_models_df\n",
    "    global all_models\n",
    "    global X_min_for_regress\n",
    "    global time_constant\n",
    "    global arr_rules_history\n",
    "    global model_interval\n",
    "    global w\n",
    "    if X_min_for_regress is None:\n",
    "        X_min_for_regress = X_min.copy()\n",
    "        X_min_for_regress = X_min_for_regress.drop(X_min_for_regress.index).reset_index(drop=True) \n",
    "        X_min_for_regress = X_min_for_regress.append(X_min, ignore_index=True) \n",
    "    else:\n",
    "        X_min_for_regress = X_min_for_regress.append(X_min, ignore_index=True)    \n",
    "    \n",
    "\n",
    "    if window_counter < model_interval:\n",
    "        return\n",
    "        \n",
    "        \n",
    "    if window_counter % model_interval != 0:\n",
    "        return\n",
    "    \n",
    "    for index, row in all_models_df.iterrows():\n",
    "        all_models_df.at[index,'weight'] = round(sigmoid_decay((window_counter - row['window'])/model_interval) * row['weight'] , 4)\n",
    "    all_models_df = all_models_df[all_models_df['weight'] > 0.00]\n",
    "    #print(\"total rows after removal:\",len(all_models_df))   \n",
    "\n",
    "    for ep in range(0,epochs):\n",
    "        #generate a random number atleast more than 2\n",
    "        if(n_features > 12):\n",
    "            k = random.randint(2, 10)        \n",
    "        else:\n",
    "            k = random.randint(2, n_features)        \n",
    "        epochs_1 = 1\n",
    "        for ep1 in range (0,epochs_1):\n",
    "            #print(\"inner_episode\")\n",
    "            n_f = random.sample(range(0,n_features),k)\n",
    "            \n",
    "            X_min_c = X_min_for_regress.iloc[:,n_f]\n",
    "            X_min_c = X_min_c.dropna()  \n",
    "            \n",
    "            if(len(X_min_c)<4):\n",
    "                continue\n",
    "            #print(\"len:\",X_min_c)\n",
    "            \n",
    "            for f in range (0,len(n_f)):\n",
    "                ind_list = n_f.copy()\n",
    "                ind_list.pop(f)\n",
    "                ind_list = [element + 1 for element in ind_list]\n",
    "                dep = n_f[f]+1\n",
    "                ind_list = list(map(str, ind_list))\n",
    "                X = X_min_c[ind_list]\n",
    "                y = X_min_c[str(dep)]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "                \n",
    "\n",
    "                # Create and fit a linear regression model\n",
    "                model = LinearRegression()\n",
    "                model.fit(X, y)\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                #y_test = y\n",
    "                y_test = y_test.values\n",
    "                # Evaluate the model\n",
    "                \n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                weight = (0.5 * r2) + 0.5 * (len(X)/model_interval)\n",
    "                all_models_df[\"rmse\"] = all_models_df[\"rmse\"].astype(float)\n",
    "                all_models_df[\"dep\"] = all_models_df[\"dep\"].astype(int)\n",
    "                model_id = str(uuid.uuid4())\n",
    "                all_models_df = all_models_df.append({'model_id' : model_id,'ind' : ','.join(ind_list), 'dep' : str(dep), \n",
    "                                            'mse' : str(mse), 'rmse' : str(rmse), 'r2' : str(r2), 'window' : window_counter, 'life' : 1, \n",
    "                                                       'coef' : model.coef_, 'intercept' : model.intercept_, 'weight' : weight},ignore_index = True)\n",
    "                all_models[model_id] = model\n",
    "                    \n",
    "                all_models_df[\"rmse\"] = all_models_df[\"rmse\"].astype(float)\n",
    "                all_models_df[\"dep\"] = all_models_df[\"dep\"].astype(int)\n",
    "    X_min_for_regress = None\n",
    "    arr_rules_history.append(len(all_models_df))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "def cluster_classifiers(df_clfs):\n",
    "    global diversity_lambda\n",
    "    global weight_lambda\n",
    "\n",
    "    data_scaled = df_clfs\n",
    "    data_scaled = preprocessing.scale(df_clfs)\n",
    "    max_weight_epochs = 0\n",
    "    indexes_for_cluster = []\n",
    "    for a in range (0,1):\n",
    "        #k = random.randint(3, 6)\n",
    "        #print(\"in pruning:\",df_clfs)\n",
    "        features = df_clfs.drop(columns=['index','weight','final_weight'], axis=1)\n",
    "        data_scaled = features\n",
    "        \n",
    "        try:\n",
    "            data_scaled = preprocessing.scale(features)\n",
    "            linked = linkage(data_scaled, 'average')\n",
    "            last = linked[-10:, 2]\n",
    "            last_rev = last[::-1]\n",
    "            idxs = np.arange(1, len(last) + 1)\n",
    "            accele = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "            accele_rev = accele[::-1]\n",
    "            k = accele_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "            \n",
    "        except:\n",
    "            print(\"k is random\")\n",
    "            k = random.randint(3, 6)\n",
    "\n",
    "        \n",
    "        #print(\"k:\",k)\n",
    "        \n",
    "        model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='average', memory=None, n_clusters=k)\n",
    "        df_clfs['cluster'] = model.fit_predict(features)\n",
    "       \n",
    "        \n",
    "        # Step 1: Calculate the average of dq, ddf, and dda\n",
    "        average_values_div = df_clfs[[ 'dq','ddf', 'dda']].mean(axis=1)\n",
    "\n",
    "        # Step 2: Multiply the average by 0.5\n",
    "        average_values_div *= diversity_lambda\n",
    "\n",
    "        #print(\"divresity averages:\",average_values_div)\n",
    "        \n",
    "        # Step 3: Multiply weight by 0.5\n",
    "        df_clfs['weight'] *= weight_lambda\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Step 4: Sum the results and assign to final_weight\n",
    "        df_clfs['final_weight'] = average_values_div + df_clfs['weight']\n",
    "        \n",
    "\n",
    "        # Assuming your DataFrame is named df\n",
    "        result = df_clfs.groupby('cluster')['final_weight'].mean().reset_index()\n",
    "        # Display the result\n",
    "        #print(\"result:\",result)\n",
    "        \n",
    "        max_cluster = result.loc[result['final_weight'].idxmax(), 'cluster']\n",
    "\n",
    "        weight_found = result.loc[max_cluster,'final_weight']\n",
    "        if (weight_found > max_weight_epochs):\n",
    "            max_weight_epochs = weight_found\n",
    "            indexes_for_cluster = df_clfs[df_clfs['cluster'] == max_cluster].index.tolist()\n",
    "            indexes_for_cluster = np.array(indexes_for_cluster)\n",
    "            weights = df_clfs.loc[indexes_for_cluster,'final_weight']\n",
    "        \n",
    "    return np.array(indexes_for_cluster),weights.values\n",
    "        \n",
    "    \n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    data = df.values\n",
    "    return data, data[:, :-1], data[:, -1]\n",
    "\n",
    "def delete_files_from_folder(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "\n",
    "# Function to convert tuples to strings and remove parentheses and trailing commas\n",
    "def convert_tuple_to_string(tup):\n",
    "    return str(tup).lstrip('(').rstrip(')').rstrip(',')\n",
    "\n",
    "\n",
    "\n",
    "def custom_compare(str1, str2):\n",
    "    num_part1, rest1 = ''.join(filter(str.isdigit, str1)), ''.join(filter(lambda x: not x.isdigit(), str1))\n",
    "    num_part2, rest2 = ''.join(filter(str.isdigit, str2)), ''.join(filter(lambda x: not x.isdigit(), str2))\n",
    "\n",
    "    # Compare numerical parts\n",
    "    if int(num_part1) != int(num_part2):\n",
    "        return False\n",
    "\n",
    "    # Compare remaining parts lexicographically\n",
    "    return rest1 == rest2\n",
    "\n",
    "def impute_row(missing_row,training_impute):\n",
    "    global col_means\n",
    "    global total_miss_count\n",
    "    global imputed_from_arm\n",
    "    global arr_ord_cols\n",
    "    global all_models_df\n",
    "    global all_models\n",
    "    # Check for NaN values\n",
    "    contains_nan = np.isnan(missing_row).any()\n",
    "    global arr_ord_cols\n",
    "    \n",
    "    if contains_nan:  \n",
    "        \n",
    "        if(np.isnan(col_means).any()):\n",
    "            print(\"col means (during impute null):\",col_means)\n",
    "        if(len(all_models_df) == 0):\n",
    "            for i in range(0,len(missing_row)):\n",
    "                if(np.isnan(missing_row[i])):\n",
    "                    val = col_means[i]  \n",
    "                    if(i in arr_ord_cols):\n",
    "                        val = int(val)\n",
    "                    missing_row[i] = val                  \n",
    "                    if training_impute:\n",
    "                        #imputed_from_arm += 1\n",
    "                        total_miss_count += 1\n",
    "                        \n",
    "            return missing_row\n",
    "        \n",
    "      \n",
    "        missing_row = missing_row.tolist()\n",
    "        row_df = pd.DataFrame({'data': missing_row})\n",
    "        row_df_org = row_df\n",
    "        l = -1\n",
    "        for index, row in row_df_org.iterrows():            \n",
    "            l+=1\n",
    "            if(pd.isnull(row[0])):\n",
    "                #print(\"index is nan:\",index)\n",
    "                if training_impute:\n",
    "                    #imputed_from_arm += 1\n",
    "                    total_miss_count += 1\n",
    "                \n",
    "                #get non nan columns of this row\n",
    "                str_list_non_null_missing_row = \"\"\n",
    "                for index1, row1 in row_df_org.iterrows():\n",
    "                    if( pd.isnull(row1[0]) == False):\n",
    "                        str_list_non_null_missing_row += str(index1) + \",\"\n",
    "                \n",
    "                pred_arr = []\n",
    "                weights_arr = []\n",
    "                pred_count = 0\n",
    "                    \n",
    "                #get all models fit for this feature\n",
    "                df_models = all_models_df[all_models_df['dep'] == l+1]\n",
    "                for index_model, row_model in df_models.iterrows(): \n",
    "                    #print(row_model)\n",
    "                    ind_list = row_model['ind']\n",
    "                    coef = row_model['coef']\n",
    "                    intercept = row_model['intercept']\n",
    "                    ind_list_arr = ind_list.split(\",\")\n",
    "                    rmse = row_model['rmse']\n",
    "                    rtwo = float(row_model['weight'])\n",
    "                    #get the model of this ID\n",
    "                    model = all_models[row_model['model_id']]\n",
    "                    flag = True\n",
    "                    pred = 0\n",
    "                    for h in range (0, len(ind_list_arr)):\n",
    "                        ind_int_val = int(ind_list_arr[h])\n",
    "                        ind_str_val = str(ind_int_val - 1)\n",
    "                        if (ind_str_val in str_list_non_null_missing_row):\n",
    "                            cof = coef[h]\n",
    "                            feature_val = str(row_df_org.iloc[ind_int_val-1,0])\n",
    "                            if(np.isnan(row_df_org.iloc[ind_int_val-1,0])):                            \n",
    "                                flag = False\n",
    "                            else:\n",
    "                                v = coef[h] * row_df_org.iloc[ind_int_val-1,0]\n",
    "                                pred += v \n",
    "                            #continue\n",
    "                        else:\n",
    "                            flag = False\n",
    "                        \n",
    "                    if(flag):            \n",
    "                        pred += intercept\n",
    "                        pred_arr.append(pred)\n",
    "                        weights_arr.append(rtwo)\n",
    "                    \n",
    "                if(len(pred_arr) > 0):\n",
    "                    pred = 0\n",
    "                    try:\n",
    "                        pred = np.average(pred_arr,axis=0,weights=weights_arr)\n",
    "                        #pred = np.average(pred_arr,axis=0 )\n",
    "                    except:\n",
    "                        print(\"norm error\")\n",
    "                        pred = np.average(pred_arr,axis=0)\n",
    "                    if(l+1 in arr_ord_cols):\n",
    "                        pred = int(pred)\n",
    "                    row_df_org.at[l,'data'] = pred \n",
    "                    if training_impute:\n",
    "                        imputed_from_arm += 1\n",
    "\n",
    "                if(len(pred_arr) == 0):\n",
    "                    # Set the new value in the DataFrame\n",
    "                    pred = col_means[l] \n",
    "                    if(l+1 in arr_ord_cols):\n",
    "                        pred = int(pred)\n",
    "                    row_df_org.at[l,'data'] = pred \n",
    "                  \n",
    "        a0 = row_df_org.iloc[:,0].values\n",
    "        return a0\n",
    "    else:\n",
    "        return missing_row\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#*********************************  Main ******************************** #\n",
    "\n",
    "global imbalance_threshold\n",
    "global arr_recall_0\n",
    "global arr_recall_1\n",
    "global arr_gmean\n",
    "global arr_accuracy\n",
    "global arr_kappa\n",
    "global restart\n",
    "global restart_threshold\n",
    "global f1_overlap_value\n",
    "global maj_weight\n",
    "global min_weight\n",
    "global ens_weight\n",
    "global arr_ord_cols\n",
    "global preq_Qtable\n",
    "global restart_threshold\n",
    "global min_life_max\n",
    "global min_life_decay_factor\n",
    "global output_folder\n",
    "global stream_folder\n",
    "\n",
    "global ensemble_pool_size\n",
    "global pruned_ensemble_size\n",
    "global rl_pruned_ids\n",
    "global learning_rate\n",
    "global decay_factor\n",
    "global minority_buffer_x\n",
    "global minority_buffer_y\n",
    "global df_buffer_x\n",
    "global rl_each_step_episodes\n",
    "global arr_elapsedtime\n",
    "\n",
    "global col_means\n",
    "global all_models_df\n",
    "global all_models\n",
    "\n",
    "\n",
    "global total_miss_count\n",
    "global imputed_from_arm\n",
    "global rules_decay_factor\n",
    "global diversity_lambda\n",
    "global weight_lambda\n",
    "\n",
    "global min_support\n",
    "global min_confidence\n",
    "global arr_rules_history\n",
    "global initial_value\n",
    "global final_value\n",
    "global time_constant\n",
    "\n",
    "global arr_all_gmeans\n",
    "global arr_all_recall0\n",
    "global arr_all_recall1\n",
    "global arr_all_perc_replaced\n",
    "\n",
    "global str_settings\n",
    "global df_weights_history\n",
    "global win_weight_count\n",
    "\n",
    "\n",
    "arr_all_gmeans = []\n",
    "arr_all_recall0 = []\n",
    "arr_all_recall1 = []\n",
    "arr_all_perc_replaced = []\n",
    "global imputed_missing_data\n",
    "\n",
    "global X_min_for_regress\n",
    "global model_interval\n",
    "\n",
    "iterations =6\n",
    "for k in range (4,iterations):\n",
    "    arr_elapsedtime = []\n",
    "    arr_rules_history = []\n",
    "    df_buffer_x = pd.DataFrame()\n",
    "    f1_overlap_value = 0\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    cwd = \"/home/temp\"\n",
    "    #cwd = \"D:\\\\usman-data\"\n",
    "    output_folder = cwd +  '/output/DRDVEN-V1/main4/'\n",
    "    stream_folder = cwd +  '/streams/mvi-v1/csvs/missing-real/'\n",
    "\n",
    "    imputed_missing_data = []\n",
    "\n",
    "    ensemble_pool_size = 15 #ensemble pool size\n",
    "    pruned_ensemble_size =8 #pruned ensemble size\n",
    "\n",
    "\n",
    "    n_train = 400 #no of instances to train the model (offline)\n",
    "    w =400 #batch size\n",
    "    overlap_threshold_f1 = 0.0 #overlap threshold\n",
    "    imbalance_ratio_threshold =1.2 #IR threshold\n",
    "    disjuncts_threshold =2 #MinD Threshold\n",
    "    disjunct_threshold_perc_maj = 1.0 #overlap removal threshold\n",
    "    maj_weight = 0.1 #lambda_0 majority weight in ensemble\n",
    "    min_weight = 0.9  #Lambda_1 minority weight in ensemble\n",
    "    min_life_max = 0.99 #minoirty instance life max value\n",
    "    min_life_decay_factor = 0.5 #minority instance life decay factor\n",
    "\n",
    "    diversity_lambda = 0\n",
    "    weight_lambda = 1\n",
    "    \n",
    "    #sigmoid delay\n",
    "    initial_value = 1.0  # Initial value\n",
    "    final_value = 0.0  # Final value\n",
    "    time_constant = 0.75 # Time constant controlling the rate of decay for linear models\n",
    "    time_constant_ensemble_weight =0.75 #ime constant controlling the rate of decay for ensemble weight\n",
    "    \n",
    "    model_interval = 5\n",
    "    win_weight_count = 10\n",
    "    \n",
    "    arr_ord_cols= []\n",
    "    #arr_ord_cols = [5,6,7,8,9,10] #1-based index  GMSC\n",
    "    #arr_ord_cols = [13,14,15,16,17,18,19,20,21] #1-based index  IJCNN1\n",
    "    #arr_ord_cols = [10,11,12] #1-based index  LOAN\n",
    "    #arr_ord_cols = [1,2,3,4,5,6,7,8,9,10] #1-based index  poker\n",
    "    #arr_ord_cols = [6,27,28,29] #1-based index  twitter\n",
    "    #arr_ord_cols = [1,2] #1-based index  mixed\n",
    "    #arr_ord_cols = [8,28,29,30] #1-based index  trip adv\n",
    "    #arr_ord_cols = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28] #1-based index pakdd\n",
    "    #arr_ord_cols = [2,3,4,5,6,7,8,9,10,11,12,13,19] #1-based index hepatitis\n",
    "    \n",
    "    X_min_for_regress = None\n",
    "    \n",
    "    #filename = \"noaa.csv\"\n",
    "    time_constant = 1.5 # Time constant controlling the rate of decay for linear models\n",
    "    time_constant_ensemble_weight = 1.5 #ime constant controlling the rate of decay for ensemble weight\n",
    "    \n",
    "    f_arr = filename.split(\".\")\n",
    "\n",
    "    filename = stream_folder  + filename\n",
    "\n",
    "    Xy, X, y = read_data(filename)\n",
    "    # Set x,y as numeric\n",
    "    X = X.astype(float)\n",
    "    n_samples, n_features = X.shape\n",
    "    print(n_samples)\n",
    "    print(n_features)\n",
    "    ups =  np.count_nonzero(y == 1)\n",
    "    downs = np.count_nonzero(y == 0)\n",
    "    print(\"1s:\",str(ups/n_samples))\n",
    "    print(\"0s:\",str(downs/n_samples))\n",
    "\n",
    "    #create the minority buffer structure\n",
    "\n",
    "    cols_str = []\n",
    "    for f in range(1,n_features+1):\n",
    "        cols_str.append(str(f))\n",
    "    cols_str.append(\"life\")\n",
    "    cols_str.append(\"recall_weight\")\n",
    "    cols_str.append(\"total_weight\")\n",
    "    df_buffer_x = pd.DataFrame(columns = cols_str)\n",
    "    #print(\"created\")\n",
    "    #print(df_buffer_x)\n",
    "    \n",
    "    \n",
    "    cols_str = []\n",
    "    cols_str.append('i') \n",
    "    for f in range(1,win_weight_count+1):\n",
    "        cols_str.append(str(f))    \n",
    "    df_weights_history = pd.DataFrame(columns = cols_str)\n",
    "\n",
    "    \n",
    "    all_models = {}\n",
    "    all_models_df = pd.DataFrame( columns =['model_id','ind','dep','mse','rmse','r2','window','life','coef','intercept','weight'])\n",
    "\n",
    "  \n",
    "    arr_recall_0= []\n",
    "    arr_recall_1= []\n",
    "    arr_gmean= []\n",
    "    arr_accuracy= []\n",
    "    arr_kappa= []\n",
    "    rl_pruned_ids = []\n",
    "\n",
    "    col_means = []\n",
    "    minority_buffer_x = []\n",
    "    minority_buffer_y = []    \n",
    "    \n",
    "    total_miss_count = 0 \n",
    "    imputed_from_arm = 0\n",
    "\n",
    " \n",
    "    file_out_prefix = \"BININI_\" + str(f_arr[0])\n",
    "    print(file_out_prefix)\n",
    "    print(\"iteration:\",k) \n",
    "    clfs = [DetectorClassifier(HoeffdingTreeClassifier(),[0,1])]\n",
    "    clfs_label = [\"Hoeffding Tree Classifier\"]\n",
    "\n",
    "    print(\"Features Originally:\", n_features)\n",
    "\n",
    "    for i in range(len(clfs)):\n",
    "        #print(\"\\n{}:\".format(clfs_label[i]))\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            #print(\"Sending Features\", n_features)\n",
    "            print(disjunct_threshold_perc_maj)\n",
    "            #print(\"....\")\n",
    "            file_out_prefix1 = file_out_prefix + \"_\" + str(k)\n",
    "            r0,r1,gm = prequential(w,Xy,X, y, clfs[i], n_features,overlap_threshold_f1,\n",
    "                        imbalance_ratio_threshold,disjunct_threshold_perc_maj,file_out_prefix1,n_train)\n",
    "            arr_all_gmeans.append(gm)\n",
    "            arr_all_recall0.append(r0)\n",
    "            arr_all_recall1.append(r1)\n",
    "    #print(file_out_prefix)\n",
    "    #print(\"rules generated:\",len(all_rules_df))\n",
    "    #print(\"total missing:\",total_miss_count)\n",
    "    #print(\"imputed with arm:\",imputed_from_arm)\n",
    "    if(total_miss_count >0):\n",
    "        print(\"% replaced:\",round(100*(imputed_from_arm/total_miss_count),2))\n",
    "        arr_all_perc_replaced.append(round(100*(imputed_from_arm/total_miss_count),2))\n",
    "    else:\n",
    "        arr_all_perc_replaced.append(0)\n",
    "    total_miss_count = 0 \n",
    "    imputed_from_arm = 0\n",
    "    print(\"Current Avg Recall0 of all iterations:\",np.mean(arr_all_recall0))\n",
    "    print(\"Current Avg Recall1 of all iterations:\",np.mean(arr_all_recall1))\n",
    "    print(\"Current Avg GM of all iterations:\",np.mean(arr_all_gmeans))\n",
    "    #np.savetxt(output_folder + file_out_prefix + \"-\" + str(k) +'-ruleshistory.csv',  arr_rules_history, delimiter=',',fmt='%1.2f')\n",
    "    \n",
    "\n",
    "str_results = \"\"\n",
    "str_results += \"\\n\\n\\n--------All Results------\"\n",
    "str_results += \"\\n\" + file_out_prefix + \"\\n\\n\"\n",
    "for a in range (0,len(arr_all_gmeans)):    \n",
    "    str_results += \"\\nIteration:\"+ str(a)\n",
    "    str_results += \"\\nRecall0:\"+ str(arr_all_recall0[a])\n",
    "    str_results += \"\\nRecall1:\"+ str(arr_all_recall1[a])\n",
    "    str_results += \"\\nGM:\"+ str(arr_all_gmeans[a])\n",
    "    str_results += \"\\nReplaced %\"+ str(arr_all_perc_replaced[a])\n",
    "\n",
    "str_results += \"\\n------\"\n",
    "\n",
    "str_results += \"\\nIterations:\" + str(iterations)\n",
    "str_results += \"\\nAvg Recall0:\"+ str(np.mean(arr_all_recall0))\n",
    "str_results += \"\\nAvg Recall1:\"+ str(np.mean(arr_all_recall1))\n",
    "str_results += \"\\nAvg GM:\"+ str(np.mean(arr_all_gmeans))\n",
    "str_results += \"\\nAvg Replaced %\"+ str(np.mean(arr_all_perc_replaced))\n",
    "#with open(output_folder + file_out_prefix+ \"-\"  + '-results.txt','w') as file:\n",
    "    #file.write(str_results)\n",
    "    \n",
    "print(str_results)\n",
    "sys.exit()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309dba4-e957-4390-b319-7f9b61e32243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d5902-e27a-40dd-89b2-02236107b7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
